# -*- coding: utf-8 -*-
"""Chatbot using RAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hAoM7lM-hm9nRkl6TP81QJESJ0nnLU1_
"""

!pip install pinecone

!pip install openai transformers sentence-transformers

!pip install pdfplumber

import pdfplumber

def extract_text_from_pdf(pdf_path):
    text = ""
    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            text += page.extract_text() + "\n"
    return text

pdf_text = extract_text_from_pdf("parsed_output.pdf")

def chunk_text(text, chunk_size=300, overlap=50):
    words = text.split()
    chunks = []
    start = 0
    while start < len(words):
        end = start + chunk_size
        chunk = " ".join(words[start:end])
        chunks.append(chunk)
        start += chunk_size - overlap
    return chunks

chunks = chunk_text(pdf_text)

from sentence_transformers import SentenceTransformer
import pinecone
from pinecone import Pinecone, ServerlessSpec

# Embed chunks
embedder = SentenceTransformer("BAAI/bge-large-en", device='cuda')
embeddings = embedder.encode(chunks)

pc = Pinecone(api_key="pcsk_6tpvoz_GoSubH6Ff4hTfMLByd5MQF74xEb2vkQnrKstz6ER71xfVm2DRu7h8hvrswN8ExJ")

# Connect to index
index = pc.Index("medicalbooks")


# Prepare and upsert
vectors = [(f"chunk-{i}", emb.tolist(), {"text": chunks[i]}) for i, emb in enumerate(embeddings)]
index.upsert(vectors)

index = pc.Index("medicalbooks")

embedder = SentenceTransformer("BAAI/bge-large-en", device='cuda')

from sentence_transformers import SentenceTransformer
import pinecone
from pinecone import Pinecone, ServerlessSpec
pc = Pinecone(api_key="pcsk_6tpvoz_GoSubH6Ff4hTfMLByd5MQF74xEb2vkQnrKstz6ER71xfVm2DRu7h8hvrswN8ExJ")

def retrieve_context(query, top_k=3):
    query_vec = embedder.encode([query])[0].tolist()
    results = index.query(vector=query_vec, top_k=top_k, include_metadata=True)
    return [match["metadata"]["text"] for match in results["matches"]]

from openai import OpenAI

client = OpenAI(
    api_key="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",
    base_url="https://api.groq.com/openai/v1"
)


def generate_rag_response(user_query):
    context_chunks = retrieve_context(user_query)
    context = "\n".join(context_chunks)
    prompt = f"Context:\n{context}\n\nUser Question: {user_query}\n\nAnswer:"

    response = client.chat.completions.create(
        model="deepseek-r1-distill-llama-70b",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.7
    )
    return response.choices[0].message.content

question = "For Meglitinide formulation what is the drug prescribed and minimum dosage?"
response = generate_rag_response(question)
print("Bot:", response)