# -*- coding: utf-8 -*-
"""risk prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ab6ydd98LQdi8mtS_0aCvfZNUKoKyvCE
"""

import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.feature_selection import SelectKBest, f_regression
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LassoCV

# 1. Load your data
df = pd.read_csv("Combined_With_New_Features.csv", encoding='latin1')

# 2. Drop any remaining rows with NA in your predictor or target columns
df = df.dropna()

# 3. Define X and y
y = df['HbA1c3']
X = df.drop(columns=['HbA1c3', 'Group', 'DDS3', 'FVG3', 'Reduction (%)', 'INSULIN REGIMEN', 'Date1', 'Date2', 'GENDER', 'CKD Stage'])    # drop the target, any non-numeric labels and non relevent columns

# 4. Univariate ranking (ANOVA F‐test)
uni = SelectKBest(score_func=f_regression, k='all')
uni.fit(X, y)
uni_scores = pd.Series(uni.scores_, index=X.columns).sort_values(ascending=False)
print("=== Univariate F‐scores ===")
print(uni_scores.head(10), "\n")

# 5. Model‐based importances (Random Forest)
rf = RandomForestRegressor(n_estimators=200, random_state=42)
rf.fit(X, y)
rf_imp = pd.Series(rf.feature_importances_, index=X.columns).sort_values(ascending=False)
print("=== Random Forest importances ===")
print(rf_imp.head(10), "\n")

# 6. Lasso for sparse linear selection
lasso = LassoCV(cv=5, random_state=42).fit(X, y)
lasso_coef = pd.Series(lasso.coef_, index=X.columns)
lasso_coef = lasso_coef.reindex(lasso_coef.abs().sort_values(ascending=False).index)
print("=== Lasso coefficients ===")
print(lasso_coef.head(10), "\n")

# 7. (Optional) Cross‐validation score of RF on top k features
top3 = rf_imp.head(4).index.tolist()
score = cross_val_score(rf, X[top3], y, cv=5, scoring='r2').mean()
print(f"RF R² using top 3 features ({top3}): {score:.3f}")

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV
from sklearn.svm import SVR
from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor
from sklearn.metrics import r2_score, mean_absolute_error

# ─── CONFIG ────────────────────────────────────────────────────────────────────
DATA_PATH = "Combined_With_New_Features.csv"
FEATURES = [
    'HbA1c2',
    'A1c_pct_change_1to2',
    'FVG2'
]
TARGET = 'HbA1c3'
TEST_SIZE = 0.2
RANDOM_STATE = 42

# ─── 1. LOAD & CLEAN ─────────────────────────────────────────────────────────────
df = pd.read_csv(DATA_PATH, encoding='latin1')
# Coerce selected columns to numeric and drop any rows with NaN in them
all_cols = FEATURES + [TARGET]
df[all_cols] = df[all_cols].apply(pd.to_numeric, errors='coerce')
df = df.dropna(subset=all_cols)

X = df[FEATURES]
y = df[TARGET]

# ─── 2. TRAIN/VALIDATION SPLIT ─────────────────────────────────────────────────
X_train, X_val, y_train, y_val = train_test_split(
    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE
)

# ─── 3. MODEL DICTIONARY ─────────────────────────────────────────────────────────
models = {
    "Linear Regression": LinearRegression(),
    "RidgeCV": RidgeCV(cv=5),
    "LassoCV": LassoCV(cv=5, random_state=RANDOM_STATE),
    "SVR (RBF)": Pipeline([
        ('scaler', StandardScaler()),
        ('svr', SVR(kernel='rbf', C=1.0, gamma='scale'))
    ]),
    "Random Forest": RandomForestRegressor(
        n_estimators=100, max_depth=5, random_state=RANDOM_STATE
    ),
    "HistGBM": HistGradientBoostingRegressor(
        max_iter=100, random_state=RANDOM_STATE
    )
}

# ─── 4. TRAIN & EVALUATE ─────────────────────────────────────────────────────────
results = []
for name, model in models.items():
    model.fit(X_train, y_train)
    preds = model.predict(X_val)
    results.append({
        'Model': name,
        'R2': r2_score(y_val, preds),
        'MAE': mean_absolute_error(y_val, preds)
    })

# ─── 5. REPORT ──────────────────────────────────────────────────────────────────
results_df = pd.DataFrame(results).sort_values('R2', ascending=False)
print(results_df.to_string(index=False))

# Install and import dependencies
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import cross_val_score
from sklearn.feature_selection import SelectKBest, f_regression
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LassoCV
from sklearn.ensemble import GradientBoostingRegressor, ExtraTreesRegressor
from sklearn.linear_model import RidgeCV
from sklearn.feature_selection import mutual_info_regression

#  Load the uploaded CSV
df = pd.read_csv("dv3.csv", encoding='ISO-8859-1')

#  Data Cleaning
df = df.dropna()

# Define predictors (X) and target (y)
y = df['HbA1c3']
X = df.drop(columns=[
    'HbA1c3', 'Group', 'DDS3', 'FVG3', 'Reduction (%)',
    'INSULIN REGIMEN', 'Date1', 'Date2', 'GENDER', 'CKD Stage', 'eGFR_stage_label'
])

#  1. Univariate F-test
uni = SelectKBest(score_func=f_regression, k='all')
uni.fit(X, y)
uni_scores = pd.Series(uni.scores_, index=X.columns)

#  2. Random Forest
rf = RandomForestRegressor(n_estimators=200, random_state=42)
rf.fit(X, y)
rf_imp = pd.Series(rf.feature_importances_, index=X.columns)

#  3. Lasso Regression
lasso = LassoCV(cv=5, random_state=42).fit(X, y)
lasso_coef = pd.Series(lasso.coef_, index=X.columns)

# 1. Gradient Boosting
gbm = GradientBoostingRegressor(n_estimators=200, random_state=42)
gbm.fit(X, y)
gbm_imp = pd.Series(gbm.feature_importances_, index=X.columns)

# 2. Extra Trees
et = ExtraTreesRegressor(n_estimators=200, random_state=42)
et.fit(X, y)
et_imp = pd.Series(et.feature_importances_, index=X.columns)

# 3. Ridge Regression
ridge = RidgeCV(cv=5).fit(X, y)
ridge_coef = pd.Series(ridge.coef_, index=X.columns)

# 4. Mutual Information
mi = pd.Series(mutual_info_regression(X, y, random_state=42), index=X.columns)

# Combine all into DataFrame
feature_df = pd.DataFrame({
    'F-test': uni_scores,
    'Random Forest': rf_imp,
    'Gradient Boosting': gbm_imp,
    'Extra Trees': et_imp,
    'Lasso': lasso_coef,
    'Ridge': ridge_coef,
    'Mutual Info': mi
}).dropna()

top_n = 10
top_features = pd.concat([
    feature_df['F-test'].nlargest(top_n),
    feature_df['Random Forest'].nlargest(top_n),
    feature_df['Lasso'].abs().nlargest(top_n),
    feature_df['Ridge'].abs().nlargest(top_n),
    feature_df['Gradient Boosting'].nlargest(top_n),
    feature_df['Extra Trees'].nlargest(top_n),
    feature_df['Mutual Info'].nlargest(top_n),
]).index.unique()

filtered_df = feature_df.loc[top_features]

# Normalize for comparison
normalized_df = filtered_df.copy()
for col in normalized_df.columns:
    max_val = normalized_df[col].abs().max()
    if max_val != 0:
        normalized_df[col] = normalized_df[col] / max_val

# Heatmap of top features
plt.figure(figsize=(12, 6))
sns.heatmap(normalized_df, annot=True, cmap='viridis', linewidths=0.5)
plt.title("Top Features Across Methods (Normalized)")
plt.tight_layout()
plt.show()

#  Cross-validation using top 5 features from Random Forest
top4_rf = rf_imp.nlargest(5).index.tolist()
rf_score = cross_val_score(rf, X[top4_rf], y, cv=5, scoring='r2').mean()
print(f"Random Forest R² using top 4 features: {rf_score:.3f}")

# Install and import dependencies
import pandas as pd
import numpy as np

from sklearn.model_selection import cross_val_score
from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor
from sklearn.linear_model import LassoCV, RidgeCV

#  Load the uploaded CSV
df = pd.read_csv("dv3.csv", encoding='ISO-8859-1')

#  Data Cleaning
df = df.dropna()

#  Define predictors (X) and target (y)
y = df['HbA1c3']
X = df.drop(columns=[
    'HbA1c3', 'Group', 'DDS3', 'FVG3', 'Reduction (%)',
    'INSULIN REGIMEN', 'Date1', 'Date2', 'GENDER', 'CKD Stage', 'eGFR_stage_label'
])

#  Feature Scoring
uni = SelectKBest(score_func=f_regression, k='all')
uni.fit(X, y)
uni_scores = pd.Series(uni.scores_, index=X.columns)

rf = RandomForestRegressor(n_estimators=200, random_state=42)
rf.fit(X, y)
rf_imp = pd.Series(rf.feature_importances_, index=X.columns)

lasso = LassoCV(cv=5, random_state=42).fit(X, y)
lasso_coef = pd.Series(lasso.coef_, index=X.columns)

gbm = GradientBoostingRegressor(n_estimators=200, random_state=42)
gbm.fit(X, y)
gbm_imp = pd.Series(gbm.feature_importances_, index=X.columns)

et = ExtraTreesRegressor(n_estimators=200, random_state=42)
et.fit(X, y)
et_imp = pd.Series(et.feature_importances_, index=X.columns)

ridge = RidgeCV(cv=5).fit(X, y)
ridge_coef = pd.Series(ridge.coef_, index=X.columns)

mi = pd.Series(mutual_info_regression(X, y, random_state=42), index=X.columns)

# Combine results
feature_df = pd.DataFrame({
    'F-test': uni_scores,
    'Random Forest': rf_imp,
    'Gradient Boosting': gbm_imp,
    'Extra Trees': et_imp,
    'Lasso': lasso_coef,
    'Ridge': ridge_coef,
    'Mutual Info': mi
}).dropna()

#  Display top N features from each method
top_n = 10
print("\n=== Top Features by Method ===\n")

for method in feature_df.columns:
    print(f"\n▶ {method} (Top {top_n})")
    print(feature_df[method].abs().sort_values(ascending=False).head(top_n).to_string())

#  Evaluate Random Forest using its top 5 features
top5_rf = rf_imp.nlargest(5).index.tolist()
rf_score = cross_val_score(rf, X[top5_rf], y, cv=5, scoring='r2').mean()
print(f"\n Random Forest R² using top 5 features: {rf_score:.3f}")

df = pd.read_csv("dv3.csv", encoding='ISO-8859-1')

# Define target and predictors
target = 'HbA1c3'
drop_cols = ['Group', 'DDS3', 'FVG3', 'Reduction (%)',
             'INSULIN REGIMEN', 'Date1', 'Date2', 'GENDER', 'CKD Stage']

# Make sure all columns exist
drop_cols = [col for col in drop_cols if col in df.columns]

y = df[target]
X = df.drop(columns=[target] + drop_cols)

# Drop rows that have missing values only in X or y
df_filtered = pd.concat([X, y], axis=1).dropna()
X = df_filtered.drop(columns=[target])
y = df_filtered[target]

print("X shape:", X.shape)
print("y shape:", y.shape)

print(df.isnull().sum().sort_values(ascending=False).head(15))
print(df.head(5))

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, r2_score

# 1. Load dataset
df = pd.read_csv("dv3.csv", encoding='ISO-8859-1')
df = df.dropna()

# 2. Define target and final selected features
target = 'HbA1c3'
top_features = [
    'HbA1c2', 'HbA1c1', 'FVG1', 'FVG2', 'Avg_FVG_1_2',
    'Reduction A', 'ReductionA_per_day', 'FVG_Delta_1_2'
]

# 3. Prepare training data
X = df[top_features]
y = df[target]

# 4. Train/test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# 5. Train a Random Forest model
model = RandomForestRegressor(n_estimators=200, random_state=42)
model.fit(X_train, y_train)

# 6. Predict and evaluate
y_pred = model.predict(X_test)
r2 = r2_score(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)

# 7. Output results
print(" Model Trained with Top Features")
print(f"R² Score: {r2:.3f}")
print(f"Mean Absolute Error (MAE): {mae:.3f}")

# 8. Optional: Cross-validation score for robustness
cv_r2 = cross_val_score(model, X, y, cv=5, scoring='r2').mean()
print(f"Cross-Validated R²: {cv_r2:.3f}")

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import (
    RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor
)
from sklearn.linear_model import LassoCV, RidgeCV
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.metrics import r2_score, mean_absolute_error

# 1. Load and prepare data
df = pd.read_csv("dv3.csv", encoding='ISO-8859-1')
df = df.dropna()

# 2. Define target and top features
target = 'HbA1c3'
top_features = [
    'HbA1c2', 'HbA1c1', 'FVG1', 'FVG2', 'Avg_FVG_1_2',
    'Reduction A', 'ReductionA_per_day', 'FVG_Delta_1_2',
]

X = df[top_features]
y = df[target]

# 3. Train/Test Split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# 4. Define models
models = {
    "Random Forest": RandomForestRegressor(n_estimators=200, random_state=42),
    "Gradient Boosting": GradientBoostingRegressor(n_estimators=200, random_state=42),
    "Extra Trees": ExtraTreesRegressor(n_estimators=200, random_state=42),
    "Lasso": LassoCV(cv=5, random_state=42),
    "Ridge": RidgeCV(cv=5),
    "SVR (RBF Kernel)": Pipeline([
        ('scaler', StandardScaler()),
        ('svr', SVR(kernel='rbf'))
    ]),
    "K-Nearest Neighbors": Pipeline([
        ('scaler', StandardScaler()),
        ('knn', KNeighborsRegressor(n_neighbors=5))
    ])
}

# 5. Train & evaluate each model
print(" Model Evaluation on Top Features\n")
results = []
for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    r2 = r2_score(y_test, y_pred)
    mae = mean_absolute_error(y_test, y_pred)
    cv_r2 = cross_val_score(model, X, y, cv=5, scoring='r2').mean()

    results.append({
        'Model': name,
        'R2 Score': round(r2, 3),
        'MAE': round(mae, 3),
        'Cross-Val R2': round(cv_r2, 3)
    })

# 6. Display results
results_df = pd.DataFrame(results).sort_values(by='R2 Score', ascending=False)
print(results_df.to_string(index=False))

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score, mean_absolute_error

# 1. Load data
df = pd.read_csv("dv3.csv", encoding='ISO-8859-1')
df = df.dropna()

# 2. Select top features
top_features = [
    'HbA1c2', 'HbA1c1', 'FVG1', 'FVG2', 'Avg_FVG_1_2',
    'Reduction A', 'ReductionA_per_day', 'FVG_Delta_1_2'
]
X = df[top_features]
y = df['HbA1c3']

# 3. Train/Test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 4. Define hyperparameter grids
param_grid_rf = {
    'n_estimators': [100, 200],
    'max_depth': [None, 5, 10],
    'min_samples_split': [2, 5]
}

param_grid_gbm = {
    'n_estimators': [100, 200],
    'learning_rate': [0.05, 0.1],
    'max_depth': [3, 5]
}

param_grid_svr = {
    'svr__C': [0.5, 1.0, 10],
    'svr__gamma': ['scale', 0.1, 1]
}

# 5. Setup models
rf = RandomForestRegressor(random_state=42)
gbm = GradientBoostingRegressor(random_state=42)
svr = Pipeline([
    ('scaler', StandardScaler()),
    ('svr', SVR())
])

# 6. GridSearchCV
print(" Tuning Random Forest...")
gs_rf = GridSearchCV(rf, param_grid_rf, cv=5, scoring='r2', n_jobs=-1)
gs_rf.fit(X_train, y_train)

print(" Tuning Gradient Boosting...")
gs_gbm = GridSearchCV(gbm, param_grid_gbm, cv=5, scoring='r2', n_jobs=-1)
gs_gbm.fit(X_train, y_train)

print(" Tuning SVR...")
gs_svr = GridSearchCV(svr, param_grid_svr, cv=5, scoring='r2', n_jobs=-1)
gs_svr.fit(X_train, y_train)

# 7. Evaluate best models
models = {
    'Random Forest': gs_rf,
    'Gradient Boosting': gs_gbm,
    'SVR (RBF)': gs_svr
}

results = []
for name, gs in models.items():
    best_model = gs.best_estimator_
    y_pred = best_model.predict(X_test)
    r2 = r2_score(y_test, y_pred)
    mae = mean_absolute_error(y_test, y_pred)
    results.append({
        'Model': name,
        'Best Params': gs.best_params_,
        'R2 Score': round(r2, 3),
        'MAE': round(mae, 3)
    })

# 8. Show results
results_df = pd.DataFrame(results).sort_values('R2 Score', ascending=False)
print("\n🎯 Tuned Model Results")
print(results_df.to_string(index=False))

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.linear_model import Ridge, Lasso
from sklearn.neighbors import KNeighborsRegressor
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score, mean_absolute_error

# 1. Load and prepare data
df = pd.read_csv("dv3.csv", encoding='ISO-8859-1')
df = df.dropna()

top_features = [
    'HbA1c2', 'HbA1c1', 'FVG1', 'FVG2', 'Avg_FVG_1_2',
    'Reduction A', 'ReductionA_per_day', 'FVG_Delta_1_2',
]
X = df[top_features]
y = df['HbA1c3']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# 2. Define parameter grids
param_grids = {
    "Random Forest": {
        'n_estimators': [100, 200],
        'max_depth': [None, 5, 10],
        'min_samples_split': [2, 5]
    },
    "Gradient Boosting": {
        'n_estimators': [100, 200],
        'learning_rate': [0.05, 0.1],
        'max_depth': [3, 5]
    },
    "SVR (RBF)": {
        'svr__C': [0.5, 1.0, 10],
        'svr__gamma': ['scale', 0.1, 1]
    },
    "Ridge": {
        'alpha': [0.1, 1.0, 10.0, 100.0]
    },
    "Lasso": {
        'alpha': [0.01, 0.1, 1.0, 10.0]
    },
    "KNN": {
        'knn__n_neighbors': [3, 5, 7, 9]
    }
}

# 3. Define model pipelines
models = {
    "Random Forest": RandomForestRegressor(random_state=42),
    "Gradient Boosting": GradientBoostingRegressor(random_state=42),
    "SVR (RBF)": Pipeline([
        ('scaler', StandardScaler()),
        ('svr', SVR())
    ]),
    "Ridge": Ridge(),
    "Lasso": Lasso(max_iter=10000),
    "KNN": Pipeline([
        ('scaler', StandardScaler()),
        ('knn', KNeighborsRegressor())
    ])
}

# 4. Perform GridSearchCV
results = []
for name, model in models.items():
    print(f"🔍 Tuning {name}...")
    gs = GridSearchCV(model, param_grids[name], cv=5, scoring='r2', n_jobs=-1)
    gs.fit(X_train, y_train)

    best_model = gs.best_estimator_
    y_pred = best_model.predict(X_test)
    r2 = r2_score(y_test, y_pred)
    mae = mean_absolute_error(y_test, y_pred)

    results.append({
        'Model': name,
        'Best Params': gs.best_params_,
        'R2 Score': round(r2, 3),
        'MAE': round(mae, 3)
    })

# 5. Display results
results_df = pd.DataFrame(results).sort_values(by='R2 Score', ascending=False)
print("\n🎯 Tuned Model Results")
print(results_df.to_string(index=False))

from sklearn.linear_model import Ridge
from sklearn.model_selection import GridSearchCV
import joblib

# 1. Define Ridge parameter grid
param_grid_ridge = {
    'alpha': [0.1, 1.0, 10.0, 100.0]
}

# 2. Create and fit GridSearchCV for Ridge
ridge_model = Ridge()
gs_ridge = GridSearchCV(ridge_model, param_grid_ridge, cv=5, scoring='r2', n_jobs=-1)
gs_ridge.fit(X_train, y_train)

# 3. Save the best Ridge model
joblib.dump(gs_ridge.best_estimator_, "ridge_best_model.pkl")
print("✅ Ridge model saved to 'ridge_best_model.pkl'")

import joblib

# Load the saved model
model = joblib.load("ridge_best_model.pkl")
print("✅ Model loaded successfully")

import pandas as pd

# Input values in the exact order of features
input_data = pd.DataFrame([[
    6.2,   # HbA1c2
    8.3,   # HbA1c1
    7.9,   # FVG1
    6.6,   # FVG2
    7.25,  # Avg_FVG_1_2
    33.9,   # Reduction A
    0.3,  # ReductionA_per_day
    -1.3  # FVG_Delta_1_2

]], columns=[
    'HbA1c2', 'HbA1c1', 'FVG1', 'FVG2', 'Avg_FVG_1_2',
    'Reduction A', 'ReductionA_per_day', 'FVG_Delta_1_2'
])

# Make prediction
prediction = model.predict(input_data)
print(f"🧾 Predicted HbA1c3: {prediction[0]:.2f}")

import pandas as pd
import joblib
from sklearn.metrics import mean_absolute_error

# 1. Load the trained model
model = joblib.load('ridge_best_model.pkl')

# 2. Define the features the model was trained on
trained_features = ['HbA1c2', 'HbA1c1', 'FVG1', 'FVG2', 'Avg_FVG_1_2',
    'Reduction A', 'ReductionA_per_day', 'FVG_Delta_1_2']

# 3. Load your dataset
df = pd.read_csv('dv3.csv')  # replace with actual filename

# 4. Ensure the relevant columns are present
X = df[trained_features]
y_true = df['HbA1c3']  # actual values

# 5. Predict using the model
y_pred = model.predict(X)

# 6. Calculate MAE
mae = mean_absolute_error(y_true, y_pred)

print("✅ Prediction complete.")
print(f"📉 Mean Absolute Error (MAE): {mae:.4f}")